# [enrichment_tables.datasets]
# type = "file"
# [enrichment_tables.datasets.file]
# encoding = { type = "csv", delimiter = ";" }
# path = "/opt/tables/export-dataset-20220630-074553.csv"

[enrichment_tables.resources]
type = "file"
[enrichment_tables.resources.file]
encoding = { type = "csv", delimiter = ";" }
path = "/opt/tables/export-resource-20220630-074940.csv"

# [enrichment_tables.organizations]
# type = "file"
# [enrichment_tables.organizations.file]
# encoding = { type = "csv", delimiter = ";" }
# path = "/opt/tables/export-organization-20220630-075131.csv"

# [enrichment_tables.reuses]
# type = "file"
# [enrichment_tables.reuses.file]
# encoding = { type = "csv", delimiter = ";" }
# path = "/opt/tables/export-reuse-20220630-075154.csv"

[sources.haproxy]
type = "file"
# include = [ "/opt/logs/haproxy.log.*" ]
include = [ "/opt/logs/haproxy.log.extract_1000" ]
read_from = "beginning"
# re-read whole file at all times
ignore_checkpoints = true

[transforms.parse_syslog]
type = "remap"
inputs = ["haproxy"]
source = '''
. = parse_syslog!(.message)
'''

[transforms.parse_haproxy]
type = "remap"
inputs = [ "parse_syslog" ]
# 34.248.124.42:64539 [12/Jun/2022:00:02:42.773] DATAGOUVFR_RGS~ DATAGOUVFR_NEWINFRA/dataweb-06 0/0/2/1/+3 200 +14384 - - --NN 829/812/5/3/0 0/0 \"GET /_themes/gouvfr/Marianne-Light.f2fc65ec.woff2 HTTP/1.1\
# https://github.com/gforcada/haproxy_log_analysis/blob/501cc52334e7b9c1e3959e76073ab715a52a6898/haproxy/line.py#L15
source = '''
. |= parse_regex!(
    .message,
    r'(?P<client_ip>[a-fA-F\d+\.:]+):(?P<client_port>\d+)\s+\[(?P<accept_date>.+)\]\s+(?P<frontend_name>.*)\s+(?P<backend_name>.*)/(?P<server_name>.*)\s+(?P<tq>-?\d+)/(?P<tw>-?\d+)/(?P<tc>-?\d+)/(?P<tr>-?\d+)/(?P<tt>\+?\d+)\s+(?P<status_code>-?\d+)\s+(?P<bytes_read>\+?\d+)\s+.*\s+(?P<act>\d+)/(?P<fe>\d+)/(?P<be>\d+)/(?P<srv>\d+)/(?P<retries>\+?\d+)\s+(?P<queue_server>\d+)/(?P<queue_backend>\d+)\s+"(?P<http_request>.*)"$'
)
.status_code = to_int!(.status_code)
'''
drop_on_error = true

[transforms.filter_requests]
type = "filter"
inputs = [ "parse_haproxy" ]
# only main datagouv backend and filter out http errors
condition = '.backend_name == "DATAGOUVFR_NEWINFRA" && .status_code >= 200 && .status_code < 400 ?? null'

[transforms.parse_http_request]
type = "remap"
inputs = [ "filter_requests" ]
# GET /0d/573351b046807d513851ffeb0121fbd885a8ccea6a9e30fb1b266f5fe761b9.csv HTTP/1.1
source = '''
. |= parse_regex!(
    .http_request,
    r'(?P<method>\w+)\s+(?P<path>.*)\s+(?P<protocol>\w+/\d\.\d)?'
)
'''

[transforms.detect_type]
type = "remap"
inputs = [ "parse_http_request" ]
# /fr/datasets/r/3a908e66-2d33-4cb8-8ce1-66a7a46950a4
# /resources/recharge-a-destination-tesla/20181130-201629/irve-tesla-destination-charging-20181130.csv
# TODO: handle org hits, reuses hits, API hits
source = '''
resource_path_match = parse_regex(.path, r'^/resources/(?P<resource_path>.*)') ?? false
resource_id_match = parse_regex(.path, r'^/\w{2}/datasets/r/(?P<resource_id>.*)') ?? false

if is_object(resource_path_match) {
  .request_type = "resource_download"
  .resource_path = resource_path_match.resource_path
  .request_api = false
} else if is_object(resource_id_match) {
  .request_type = "resource_download"
  .resource_id = resource_id_match.resource_id
  .request_api = false
} else if match(.path, r'^/\w{2}/datasets/.*') ?? false {
  .request_type = "dataset_hit"
  .request_api = false
}
'''

[transforms.route_by_type]
type = "route"
inputs = [ "detect_type" ]

  [transforms.route_by_type.route]
  resource = '.request_type == "resource_download"'
  dataset = '.request_type == "dataset_hit"'

[transforms.map_to_resource]
type = "remap"
inputs = [ "route_by_type.resource" ]
source = '''
if exists(.resource_id) {
  row = get_enrichment_table_record("resources", {"id": .resource_id}) ?? false
  .dataset_id = get(row, ["dataset.id"]) ?? null
} else if exists(.resource_path) {
  url, err = join(["https://static.data.gouv.fr/resources/", .resource_path])
  if err == null {
    row = get_enrichment_table_record("resources", {"url": url}) ?? false
    .dataset_id = get(row, ["dataset.id"]) ?? null
    .resource_id = row.id
  }
}
'''

# [transforms.map_to_dataset]
# type = "remap"
# inputs = [ "route_by_type.dataset" ]
# # TODO:

[transforms.metric_count_resources]
type = "log_to_metric"
inputs = [ "map_to_resource" ]

  [[transforms.metric_count_resources.metrics]]
  field = "resource_id"
  name = "resource_id"
  namespace = "resource"
  type = "counter"

    [transforms.metric_count_resources.metrics.tags]
      resource_id = "{{resource_id}}"
      dataset_id = "{{dataset_id}}"

[sinks.out]
type = "console"
inputs = [ "metric_count_resources" ]
  [sinks.out.encoding]
  codec = "json"

[sinks.influxdb_metrics]
type = "influxdb_metrics"
inputs = [ "metric_count_resources" ]
bucket = "vector-resources-bucket"
endpoint = "http://influxdb:8086/"
org = "etalab"
token = "A2jqhzcRPZeT3bAF"
default_namespace = "service"
