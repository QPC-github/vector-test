# used by `vector tap`
[api]
enabled = true
address = "0.0.0.0:8686"

[enrichment_tables.datasets]
type = "file"
[enrichment_tables.datasets.file]
encoding = { type = "csv", delimiter = ";" }
path = "/opt/tables/datasets.csv"

[enrichment_tables.resources]
type = "file"
  [enrichment_tables.resources.file]
  encoding = { type = "csv", delimiter = ";" }
  path = "/opt/tables/resources.csv"

# [enrichment_tables.organizations]
# type = "file"
# [enrichment_tables.organizations.file]
# encoding = { type = "csv", delimiter = ";" }
# path = "/opt/tables/organizations.csv"

# [enrichment_tables.reuses]
# type = "file"
# [enrichment_tables.reuses.file]
# encoding = { type = "csv", delimiter = ";" }
# path = "/opt/tables/reuses.csv"

[sources.haproxy]
type = "file"
include = [ "/opt/logs/haproxy.log.202206*" ]
# include = [ "/opt/logs/haproxy.log.extract_1000" ]
# include = [ "/opt/logs/haproxy.log.example_aggregate" ]
read_from = "beginning"
# re-read whole file at all times
# FIXME: remove on real setup
ignore_checkpoints = true

[transforms.parse_syslog]
type = "remap"
inputs = ["haproxy"]
source = '''
. = parse_syslog!(.message)
'''

[transforms.parse_haproxy]
type = "remap"
inputs = [ "parse_syslog" ]
# 34.248.124.42:64539 [12/Jun/2022:00:02:42.773] DATAGOUVFR_RGS~ DATAGOUVFR_NEWINFRA/dataweb-06 0/0/2/1/+3 200 +14384 - - --NN 829/812/5/3/0 0/0 \"GET /_themes/gouvfr/Marianne-Light.f2fc65ec.woff2 HTTP/1.1\
# https://github.com/gforcada/haproxy_log_analysis/blob/501cc52334e7b9c1e3959e76073ab715a52a6898/haproxy/line.py#L15
source = '''
. |= parse_regex!(
    .message,
    r'(?P<client_ip>[a-fA-F\d+\.:]+):(?P<client_port>\d+)\s+\[(?P<accept_date>.+)\]\s+(?P<frontend_name>.*)\s+(?P<backend_name>.*)/(?P<server_name>.*)\s+(?P<tq>-?\d+)/(?P<tw>-?\d+)/(?P<tc>-?\d+)/(?P<tr>-?\d+)/(?P<tt>\+?\d+)\s+(?P<status_code>-?\d+)\s+(?P<bytes_read>\+?\d+)\s+.*\s+(?P<act>\d+)/(?P<fe>\d+)/(?P<be>\d+)/(?P<srv>\d+)/(?P<retries>\+?\d+)\s+(?P<queue_server>\d+)/(?P<queue_backend>\d+)\s+"(?P<http_request>.*)"$'
)
.status_code = to_int!(.status_code)
'''
drop_on_error = true

[transforms.filter_requests]
type = "filter"
inputs = [ "parse_haproxy" ]
# only main datagouvfr backend and filter out http errors
condition = '.backend_name == "DATAGOUVFR_NEWINFRA" && .status_code >= 200 && .status_code < 400 ?? null'

[transforms.parse_http_request]
type = "remap"
inputs = [ "filter_requests" ]
# GET /0d/573351b046807d513851ffeb0121fbd885a8ccea6a9e30fb1b266f5fe761b9.csv HTTP/1.1
source = '''
. |= parse_regex!(
    .http_request,
    r'(?P<method>\w+)\s+(?P<path>.*)\s+(?P<protocol>\w+/\d\.\d)?'
)
'''

[transforms.detect_type]
type = "remap"
inputs = [ "parse_http_request" ]
# /fr/datasets/r/3a908e66-2d33-4cb8-8ce1-66a7a46950a4
# /resources/recharge-a-destination-tesla/20181130-201629/irve-tesla-destination-charging-20181130.csv
# TODO: handle org hits, reuses hits, API hits
source = '''
resource_path_match = parse_regex(.path, r'^/resources/(?P<resource_path>.*)') ?? false
resource_id_match = parse_regex(.path, r'^/\w{2}/datasets/r/(?P<resource_id>.*)') ?? false

if is_object(resource_path_match) {
  .request_type = "resource_download"
  .resource_path = resource_path_match.resource_path
  .request_api = false
} else if is_object(resource_id_match) {
  .request_type = "resource_download"
  .resource_id = resource_id_match.resource_id
  .request_api = false
<<<<<<< Updated upstream
} else if match(.path, r'^/\w{2}/datasets/.*') ?? false {
  .request_type = "dataset_hit"
  .request_api = false
=======
} else if is_object(resource_static_path_match) {
  .request_type = "resource_download"
  .resource_static_path = resource_static_path_match.resource_static_path
  .request_api = false
} else {
  dataset_path_match = match(.path, r'^/\w{2}/datasets/(?P<dataset_path>.*)') ?? false
  if is_object(dataset_path_match) {
    .dataset_name = dataset_path_match.dataset_path
    .request_type = "dataset_hit"
    .request_api = false
  }
>>>>>>> Stashed changes
}
'''

[transforms.route_by_type]
type = "route"
inputs = [ "detect_type" ]

  [transforms.route_by_type.route]
  resource = '.request_type == "resource_download"'
  dataset = '.request_type == "dataset_hit"'

[transforms.map_to_resource]
type = "remap"
inputs = [ "route_by_type.resource" ]
source = '''
if exists(.resource_id) {
  row = get_enrichment_table_record("resources", {"id": .resource_id}) ?? false
  .dataset_id = get(row, ["dataset.id"]) ?? null
} else if exists(.resource_path) {
  url, err = join(["https://static.data.gouv.fr/resources/", .resource_path])
  if err == null {
    row = get_enrichment_table_record("resources", {"url": url}) ?? false
    .dataset_id = get(row, ["dataset.id"]) ?? null
    .resource_id = row.id
  }
}
'''

<<<<<<< Updated upstream
=======
[transforms.filter_redirects_to_static]
type = "filter"
inputs = [ "filter_redirects_to_static" ]
# only main datagouvfr backend and filter out http errors
condition = '!(bool(.redirects_to_static) ?? false) || .method != "GET"'

[transforms.map_to_dataset]
type = "remap"
inputs = [ "route_by_type.dataset" ]
source = '''
url = join!(["http://www.data.gouv.fr/fr/datasets/", .dataset_path])
row = get_enrichment_table_record("datasets", {"url": url}) ?? {}
.dataset_id = get(row, ["id"]) ?? null
'''

>>>>>>> Stashed changes
# TODO:
# [transforms.map_to_dataset]
# type = "remap"
# inputs = [ "route_by_type.dataset" ]

# -- LOG STRAGEGY (vs metrics strategy) --
# So, this ain't pretty ðŸ˜…
# Alternatives:
# - cleanup from scratch (ie not capture groups)
# - use `only_fields` but it doesnt exist anymore ðŸ˜­
# - use log to metrics conversion (but this does not seem apt re our usecase)
#   -> see below, works pretty well
# [transforms.cleanup_message]
# type = "remap"
# inputs = ["map_to_resource"]
# source = '''
# del(.accept_date) # "12/Jun/2022:00:00:42.629",
# del(.act) # "759",
# del(.appname) # "haproxy",
# del(.backend_name) # "DATAGOUVFR_NEWINFRA",
# del(.be) # "5",
# del(.bytes_read) # "+925",
# del(.client_port) # "56940",
# del(.fe) # "742",
# del(.frontend_name) # "DATAGOUVFR_RGS~",
# del(.hostname) # "slb-04",
# del(.http_request) # "GET /fr/datasets/r/3a908e66-2d33-4cb8-8ce1-66a7a46950a4 HTTP/1.1",
# del(.message) # "47.187.205.185:56940 [12/Jun/2022:00:00:42.629] DATAGOUVFR_RGS~ DATAGOUVFR_NEWINFRA/dataweb-05 0/0/1/14/+15 302 +925 - - --NN 759/742/5/2/0 0/0 \"GET /fr/datasets/r/3a908e66-2d33-4cb8-8ce1-66a7a46950a4 HTTP/1.1\"",
# del(.procid) # 3103325,
# del(.protocol) # "HTTP/1.1"
# del(.queue_backend) # "0",
# del(.queue_server) # "0",
# del(.retries) # "0",
# del(.srv) # "2",
# del(.tc) # "1",
# del(.tq) # "0",
# del(.tr) # "14",
# del(.tt) # "+15",
# del(.tw) # "0"
# '''

# [sinks.out]
# type = "console"
# inputs = [ "cleanup_message" ]
#   [sinks.out.encoding]
#   codec = "json"

[transforms.metric_count_resources]
type = "log_to_metric"
inputs = [ "map_to_resource" ]

  [[transforms.metric_count_resources.metrics]]
  field = "resource_id"
  namespace = "resource"
  type = "counter"

    [transforms.metric_count_resources.metrics.tags]
    resource_id = "{{resource_id}}"
    dataset_id = "{{dataset_id}}"
    request_api = "{{request_api}}"
    method = "{{method}}"
    status_code = "{{status_code}}"
    # server_name = "{{server_name}}"
    # client_ip = "{{client_ip}}"

# aggregates resources count over time
# we don't _need_ to aggregate
# if we don't, metrics.set might be better than metrics.count
[transforms.aggregate_resources]
type = "aggregate"
inputs = [ "metric_count_resources" ]
interval_ms = 60_000

# [sinks.out]
# type = "console"
# inputs = [ "aggregate_resources" ]
#   [sinks.out.encoding]
#   codec = "json"

# -- LOG STRAGEGY (vs metrics strategy) --
# [sinks.influxdb_logs]
# type = "influxdb_logs"
# inputs = [ "cleanup_message" ]
# bucket = "vector-bucket"
# endpoint = "http://influxdb:8086/"
# org = "etalab"
# token = "A2jqhzcRPZeT3bAF"
# measurement = "vector-logs"

[sinks.influxdb_metrics]
type = "influxdb_metrics"
inputs = [ "metric_count_resources" ]
bucket = "vector-bucket"
endpoint = "http://influxdb:8086/"
org = "etalab"
token = "A2jqhzcRPZeT3bAF"
default_namespace = "service"
